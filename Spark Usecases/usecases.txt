---------------------------------
1. Data For Task
---------------------------------
Inceptez1.txt
I am learning Apache Spark from Inceptez Learning Resources
I am learning Apache Hadoop from Inceptez Learning Resources
I have created my technical profile at www.QuickTechie.com
I am learning Apache Spark from Training4exam Learning Resources

Accomplish the followings:-

1. Create this text file in HDFS
2. Once file is created, write the spark application which will read from HDFS as an RDD
3. Once RDD loaded, do the line count of this RDD

---------------------------------
2. Data For Task
---------------------------------
Inceptez2.txt
I am learning Apache Spark from Inceptez Learning Resources
I am learning Apache Hadoop from Inceptez Learning Resources
I have created my technical profile at www.QuickTechie.com
I am learning Apache Spark from Training4exam Learning Resources
I am learning Apache Spark from Training4exam Learning Resources

Accomplish the followings:-
1. Create this file in HDFS
2. Once file is created, write a spark application which will read this file from HDFS as an RDD
3. Filter all records contains 'Inceptez' in line and count of lines
4. Filter all records does not contains 'Inceptez' in line and count the lines

---------------------------------
3. Data For Task
---------------------------------
("We", "Are" ,"Learning" , "Hadoop" , "From" , "Inceptez" , "We", "Are" ,"Learning" , "Spark" , "From" , "Inceptez.com" , "hadoop" , "HADOOP")

Accomplish the followings:-

1. Create an RDD using using the given words
2. Once RDD is created count all the words
3. Now filter out all the words which does not have Hadoop keyword, however make sure it would count all the different cases(upper/lower) as well

---------------------------------
4. Data For Task
---------------------------------
hdpcd/Inceptez4A.txt
Inceptez.com techcert.com Incepeztchcert.com


hdpcd/Inceptez4B.txt
Hadoop Spark Scala Python Java Cloud DataScience

hdpcd/Inceptez4C.txt
India USA UK Canada Australia

Accomplish the followings:-

1. Load all 3 files in different RDDS
2. Concatnate all the data in single RDDS
3. Count all the words in of all 3 files

---------------------------------
5. Data For Task
---------------------------------
Load txns file from /home/hduser/hive/data location into RDD

Accomplish the followings:-

1. Sum of sales for texas state
2. Max number of product sales in texas
3. Sales by payment type in texas

---------------------------------
6. Data For Task
---------------------------------
Create and copy file cricket.txt in hdfs location with the data below:
Rajiv,21
Sharma,19
Kiran,22
Vijay,20
Ramesh,25
Create and copy file hockey.txt in hdfs location with the data below:
Ramesh,25
Vimal,21
Karthik,24
Vinod,22
Vijay,20
Create and copy file football.txt in hdfs location with the data below:
Ravinder,21
Ramesh,25
Karthik,24
Vijay,20
Kannan,22
Rajiv,21

Accomplish the followings:-

1. Find the players who are in cricket and football
2. Find the players who are in all 3 sports
3. Get all the distinct players
4. Get all the distinct players with sports


---------------------------------
7.	Data For Task
---------------------------------
Load the txns file into RDD

Accomplish the followings:-

1. Get totalsales by state wise and sort by state
2. Get transactioncount by date and sort by transactioncount
3. Get the date having maximum transactioncount

---------------------------------
8. Data For Task
---------------------------------
Given a file name spark6/user.csv

user.csv
id,topic,hits
Rahul,scala,120
Nikita,spark,80
Mithun,spark,1
myself,cca175,180

Accomplish the followings:-

Write spark code in scala which will remove the header part and create RDD of values as below,for all rows.
And also if id is "myself" than filter out row.


---------------------------------
9.	Data For Task
---------------------------------
You have given the follwing data in a file coursefee.txt
CourseName,Price,TaxandOthersInPercen
Hadoop,3000,10
Spark,3500,14
AWS,2700,13
Azure,2800,11
Java,3000,16
HBase,3200,20

Accomplish the followings:-

1. Load this csv file in RDD
2. Now calculate the final price using tax and add as 2 column
3. Save the final data in HDFS in location "/sparkworkouts/coursefeewithtax" with one file

Hadoop,3300,3000,10
---------------------------------
10.	Data For Task
---------------------------------

Accomplish the followings:-

1. Load the aution data(/home/hduser/sparkdata/autiondata) into the RDD. 
2. Find the total number of actions
3. Find total number of bids per itemtype
4. Finding max,min,avg number of bids among auctions
5. Save the final data in HDFS in location "/sparkworkouts/autionresult"

---------------------------------
11.	Data For Task
---------------------------------

movies.csv

1,The Nightmare Before Christmas,1993,3.9,4568
2,The Mummy,1932,3.5,4388
3,Orphans of the Storm,1921,3.2,9062
4,The Object of Beauty,1991,2.8,6150
5,Night Tide,1963,2.8,5126
6,One Magic Christmas,1985,3.8,5333
7,Muriel's Wedding,1994,3.5,6323
8,Mother's Boys,1994,3.4,5733
9,Nosferatu: Original Version,1929,3.5,5651
10,Nick of Time,1995,3.4,5333
11,Broken Blossoms,1919,3.3,5367
12,Big Night,1996,3.6,6561
13,The Birth of a Nation,1915,2.9,12118
14,The Boys from Brazil,1978,3.6,7417
15,Big Doll House,1971,2.9,5696
16,The Breakfast Club,1985,4.0,5823
17,The Bride of Frankenstein,1935,3.7,4485
18,Beautiful Girls,1996,3.5,6755
19,Bustin' Loose,1981,3.7,5598
20,The Beguiled,1971,3.4,6307
21,Born on the Fourth of July,1989,3.4,8646
22,Broadcast News,1987,3.4,7940
23,Swimming with Sharks,1994,3.3,5586
24,Beavis and Butt-head Do America,1996,3.4,4852
25,Brighton Beach Memoirs,1986,3.4,6564
26,The Best of Times,1986,3.4,6247
27,Brassed Off,1996,3.5,6040
28,Last Tango in Paris,1972,3.1,7732
29,Leprechaun 2,1994,3.2,5125
30,Incident at Oglala: The Leonard Peltier Story,1992,3.7,5487

Accomplish the followings:-

1. Load the movies data into the RDD. 
2. List the movies that having a rating greater than 4
3. List the movies that are released after 1980
4. List the movies by release year
5. Save the final data in HDFS in location "/sparkworkouts/movierelease" into 2 files


---------------------------------
12.	Data For Task
---------------------------------
You have given following 2 files:-

Content.txt
Hello this is Inceptez.com 
This is TechCert.com
Apache Spark Training
This is Spark Learning Session
Hello, is, this, the
Spark is faster than MapReduce

Remove.txt
Hello, is, this, the

Accomplish the following:-

write the spark program which reads the content.txt file and load as RDD, remove all the words from the Remove.txt(which is loaded as an RDDs of words from Remove.txt). count the occurances of each word and save it as text file in HDFS


--------------------------------
13.	Data For Task
---------------------------------
You have given following 3 files as below:-

sparkworkouts/sparkdir1/file1.txt
Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework

sparkworkouts/sparkdir2/file2.txt
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel based on the data that needs to be processed.

sparkworkouts/sparkdir3/file3.txt
his approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking

Accomplish the following:-

write the spark program which loads the all 3 file from hdfs and do the word count by filtering the following words
And result should be sorted by word count in reverse order

Filter words("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of") 

Also please make sure you load all 3 files as Single RDD(All 3 files must be loaded using single API call)

---------------------------------
14. Data For Task
---------------------------------
Given filename sparkworkouts/student.csv

data.csv
1,Lokesh
2,Bhupesh
3,Amit
4,Ratan
5,Dinesh

sparkworkouts/marks.csv
1~90~80~95
2~88~90~89
3~78~76~70
4~92~69~89
5~88~70~86


Accomplish the followings:-

1. Load the files into RDD and get the studentid,students,mark1,mark2,marks3,totalmarks
2. Get the highest mark scored studentinfo  

---------------------------------
15.	Data For Task
---------------------------------
You have given 3 csv files hdfs as below:-

EmployeeManager.csv
E01,Vishnu
E02,Satyam
E03,Shiv
E04,Sundar
E05,John
E06,Pallavi
E07,Tanvir
E08,Shekhar
E09,Vinod
E10,Jitendra

EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000

Accomplish the following:-

Using spark and its API you have to generate a joined output as below and save as text file (Separated by comma)
for final distribution and output must be sorted by id
id,name,salary,managerName

---------------------------------
16. Data For Task
---------------------------------
Given a filename EmployeeName.csv

EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

Accomplish the followings:-

Load the file from hdfs and sort it by name and save it back as (id,name) in results directory.
However make sure while saving it should be able to write in a single file

---------------------------------
17.	Data For Task
---------------------------------
You have given file spark10/sales.txt below:-

spark10/sales.txt
Department,Designation,costToCompany,State
Sales,Trainee,12000,UP
Sales,Lead,32000,AP
Sales,Lead,32000,MH
Sales,Lead,32000,TN
Sales,Lead,32000,AP
Sales,Lead,32000,TN 
Sales,Lead,32000,MH
Sales,Lead,32000,MH
Marketing,Associate,18000,TN
Marketing,Associate,18000,TN
HR,Manager,58000,TN

Accomplish the following:-

Produce the output as csv with group by department,designation,state and additional column with sum(costToCompany) and 
TotalEmployeeCount

Should get results like

Dept,Desg,State,empcount,totalcost
Sales,Lead,AP,2,64000
Sales,Lead,MH,3,96000
Sales,Lead,TN,2,64000

---------------------------------
18. Data For Task
---------------------------------
You have been given below list in scala (name,sex,cost) for each work done.

List( ("Deepak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))

Accomplish the followings:-

Now write a spark program to load this list as an RDD and do the sum of cost for combination of name and gender (as key)

---------------------------------
19.	Data For Task
---------------------------------

val a = sc.parallelize(List("dog","tiger","lion","cat","panther","eagle"))

Accomplish the following:-

Array[(Int, String)] = Array((4,lion), (5,tigereagle), (3,dogcat), (7,panther))
